# Modelagem

```{r Importa Dados Processados}
massa <- z.trn
mdl.tst <- z.tst
```

___
## Amostragem

> **Dividir os dados em Treinamento, Validação e Teste.**

  - ***Conjunto de dados de treinamento:*** a amostra de dados usada para ajustar o modelo.
  
  - ***Conjunto de dados de validação:*** é usado para avaliar um determinado modelo. Usamos esses dados para ajustar os hiperparâmetros do modelo. Portanto, o modelo ocasionalmente vê esses dados, mas nunca “aprende” com eles. Nós usamos os resultados do conjunto de validação e atualizamos os hiperparâmetros de nível superior. Portanto, a validação definida de uma maneira afeta um modelo, mas indiretamente.
  
  - ***Conjunto de dados de teste:*** a amostra de dados usada para fornecer uma avaliação imparcial de um ajuste final do modelo no conjunto de dados de treinamento. Só é usado quando um modelo é completamente treinado (usando os conjuntos de treino e validação). O conjunto de testes geralmente é o que é usado para avaliar modelos concorrentes (por exemplo, em muitas competições Kaggle, o conjunto de validação é lançado inicialmente juntamente com o conjunto de treinamento e o conjunto de testes real é lançado somente quando a competição está prestes a fechar e é o resultado do modelo no conjunto de teste que decide o vencedor).
  Muitas vezes, o conjunto de validação é usado como o conjunto de teste, mas não é uma boa prática. O conjunto de testes geralmente é bem organizado. Ele contém dados cuidadosamente amostrados que abrangem as várias classes que o modelo enfrentaria, quando usado no mundo real.
  
> ***Nota sobre validação cruzada:*** *Muitas vezes, as pessoas primeiro dividem seus conjuntos de dados em 2 - Treinar e Testar. Depois disso, eles deixam de lado o conjunto de testes e escolhem aleatoriamente X% do conjunto de dados de trem para ser o conjunto de trens real e o restante (100-X)% para o conjunto de validação , onde X é um número fixo (por exemplo, 80% ), o modelo é treinado e validado iterativamente nesses diferentes conjuntos. Existem várias maneiras de fazer isso e é comumente conhecido como Validação Cruzada. Basicamente, você usa seu conjunto de treinamento para gerar várias divisões dos conjuntos de treinamento e validação. A validação cruzada evita o ajuste e está se tornando cada vez mais popular, com a Validação Cruzada K-fold sendo o método mais popular de validação cruzada.*

**Fontes:** 

1. *[https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7)*

2. *[https://machinelearningmastery.com/difference-test-validation-datasets/](https://machinelearningmastery.com/difference-test-validation-datasets/)*

> ***Resampling***

A semente é um número criado a partir do horário atual e do ID do processo, para garantir a aleatoriedade dos resultados. Fixá-lo permite que esses resultados sejam reproduzíveis.

```{r Resampling}
# Configurando uma semente para que o Gerador Aleatório de Números seja reproduzível.
set.seed(1618)

# Criando uma coluna com índices randômicos.
massa[ ,'index'] <- ifelse(runif(nrow(massa)) < 0.8, 1, 0)

# Criando os conjuntos de treino e de validação.
mdl.trn <- massa[massa$index == 1, ]
mdl.vld <- massa[massa$index == 0, ]

# Obtem o índice (posição), no vetor de nomes, onde corresponde ao valor 'index'.
col_idx <- grep('index', names(mdl.trn))

# Remove a coluna 'index' dos datasets, utilizando a posição obtida.
mdl.trn <- mdl.trn[ , -col_idx]
mdl.vld <- mdl.vld[ , -col_idx]
```

___
## Treino
___
> Treinando um ***Modelo Linear Generalizado*** para classificação.

```{r Ajuste G.L.M.}
# Gerar Modelos de Classificação
ajt.trn <- glm(formula = Survived ~ 
                 Pclass + 
                 Sex + 
                 Age + 
                 SibSp + 
                 Parch +
                 Fare +
                 Embarked,
               family = binomial(link = "logit"), 
               data = mdl.trn)
```

**Resumo do modelo treinado:**

```{r Output G.L.M}
summary(ajt.trn)
```
___	
## Validação
___
> Validando o modelo através de predições utilizando o *Conjunto de Validação*.

```{r Predição para Validação}
pred <- predict( ajt.trn, 
                     newdata = mdl.vld,
                     type = "response" )

# Criando um dataframe com os dados observados e os preditos.
previsoes <- data.frame(observado = mdl.vld$Survived,
                        previsto = pred %>% 
                                      round() %>% 
                                      factor(labels = c("Não", "Sim")))
```

```{r Tabela Cruzada - Validação}
CrossTable(previsoes$observado, previsoes$previsto)
```

```{r Qui Quadrado - Validação}
chisq.test(previsoes$observado, previsoes$previsto)
```

```{r echo=FALSE}
hist(pred, xlab = "Predições", main = "Histograma de Predições")
```

___
### Avaliação

Para avaliar o desempenho do modelo nos dados de validação, foi construída uma *matriz de confusão*.

```{r Confusion Matrix usando Caret}
cm <- confusionMatrix(previsoes$observado, previsoes$previsto, positive = "Sim")
```

Matriz de Confusão - Usando o pacote Caret

```{r Imprime Confusion Matrix}
print(cm)
```

___
***Accuracy***

Através da *Matriz de Confusão* obtivemos a ***Acurácia de `r round((cm$table[1,1] + cm$table[2,2]) / (cm$table[1,1] + cm$table[1,2] + cm$table[2,1] + cm$table[2,2]) * 100, 2)`%***, ela corresponde a fração das premissas corretas em relação ao total de observações. Esta métrica também poderia ser calculada utilizando a função *table()* ou a função *CrossTable()*, pois ela corresponde a soma das predições corretas dividida pelo total de observações.  
Porém, não é suficiente (e nem segura) para avaliarmos a eficiência do modelo. Analisamos então, outras métricas obtidas pela Confusion Matrix, que nos informe não apenas o percentual de acertos, mas também a precisão e a sensibilidade.

___
***95% CI***

O ***Intervalo de Confiança***, denotado por *95% CI*, é ***[`r round(cm$overall["AccuracyLower"], 4)`, `r round(cm$overall["AccuracyUpper"], 4)`]***. Ele é uma *estimativa por intervalo* de um *parâmetro populacional*, que com dada frequência (*Nível de Confiança*), inclui o parâmetro de interesse. Nesse caso específico, significa dizer que 95% dos *intervalos de confiança* observados têm o valor real do parâmetro.

***No Information Rate***

***P-Value [Acc > NIR]***

***Kappa***

O ***Teste de Concordância Kappa*** foi igual a ***`r round(cm$overall["Kappa"], 4)`***. O coeficiente de Kappa tem a finalidade de medir o grau de concordância entre proporções. Ele demonstra se uma dada classificação pode ser considerada confiável.

O seu valor pode ser interpretado por meio de uma tabela.

___
***Mcnemar's Test P-Value***

___
***Sensitivity***

***Specificity***

***Pos Pred Value***

***Neg Pred Value***

***Prevalence***

***Detection Rate***

***Detection Prevalence***

***Balanced Accuracy***

```{r Plot Confusion Matrix}
fourfoldplot(cm$table)
```

___
O pacote Caret nos dá, já calculadas, as principais métricas estatísticas, baseadas nos tipos de acertos e, nos tipos de erros. Para a *Acurácia*, não havia diferenças, apenas acertos e erros.

Agora, os erros são classificados como ***"Erro do Tipo I"*** e ***"Erro do Tipo II"***. O primeiro é *baseado nos* ***Falsos Positivos (FP)***, o segundo é *baseado nos* ***Falsos Negativos (FN)***.

O **Erro do Tipo I** *(FP)*, é o resultado que tem *Significância Estatística*. A probabilidade de se cometer um *erro do tipo I* em um teste de hitótese é denominada ***Nível de Significância***, representado por ***\(\alpha\) (alfa)***. Sua interpretação é que em *\(\alpha\) vezes* (p.ex. 5%) rejeitaremos a *hipótese nula (H0)* quando ela é verdadeira.  
O **Erro do Tipo II** *(FN)*, é o ***Poder do Teste***, representado por ***\(\beta\) (beta)***. Ocorre quando a hipótese nula é falsa, mas erroneamente falhamos ao ser rejeitada.

Normalmente, ao se testar uma hipótese, é definido o *nível de significância* do *Erro do Tipo I*, chamado de α, tipicamente de 5%. Ou seja, com α = 0,05, existe 5% de chance de se rejeitar a hipótese nula, no caso dela ser verdadeira.   
- Ao se diminuir a probabilidade de ocorrer um *Erro do Tipo I*, ou seja, diminuindo o valor de α, aumenta-se a probabilidade de se ocorrer um *Erro do Tipo II*. A probabilidade da ocorrência do Erro do tipo II é chamada de β.

```{r Mosaico Confusion Matrix}
mosaicplot(cm$table, color = TRUE, shade = TRUE, main = "Mosaico para Confusion Matrix", )
```

A partir destas definições construímos as duas principais métricas, a ***Precisão*** e a ***Sensibilidade***.

A **Precisão** demonstra que de todos os casos previstos como **TP**, quantos realmente estavam certos.  
A **Sensibilidade** mostra que de todos os possíveis casos onde o target era 1, quanto o modelo conseguiu capturar.

Se *diminuírmos o erro do tipo I* (FP), tornaremos nosso modelo ***mais preciso***.  
Se *diminuírmos o erro do tipo II* (FN), tornaremos o modelo ***mais sensível***.  

Para manipular o *Erro Tipo I*...  
Para manipular o *Erro Tipo II* devemos utilizar modelos diferentes ou aumentar o tamanho da amostra.

Para alterarmos esses valores, devemos analizar as predições como uma distribuição de probabilidade, encontrando a probabilidade de cada observação pertencer a classe 1 ou 0. 

O ***Threshold*** é o percentual que definirá essa escolha, o corte que separará as classes.

A ***F1-Score*** é uma alternativa para não utilizarmos a Precisão e a Sensibilidade. Ela é uma média harmônica entre essas duas, o que torna a métrica mais sensível a desproporções.

Outra forma é a ***F-Beta*** onde podemos escolher o Peso entre Precisão e Sensibilidade através do parâmetro Beta.

___
#### Métricas de Avaliação

Avaliar um modelo de Classificação Binária:

*Acurácia (Accuracy)*, *Precisão (Precision)*, *Sensibilidade (Recall)* e *Pontuação F1 (F1-Score)*.

```{r Accuracy}
acuracia <- (cm$table[1,1] + cm$table[2,2])/(cm$table[1,1] + cm$table[1,2] + cm$table[2,1] + cm$table[2,2])

```

```{r Precision}
# Precision
precisao <- cm$table[1,1] / (cm$table[1,1] + cm$table[1,2])
```

```{r Recall}
sensibilidade <- cm$table[1,1] / (cm$table[1,1] + cm$table[2,1])
```

```{r F1-Score}
f1 <- (2 * sensibilidade * precisao) / (sensibilidade + precisao)
```

```{r Tabela de Métricas}
metricas <- kable(
              data.frame(
                "Acuracia" = acuracia, 
                "Precisao" = precisao, 
                "Sensibilidade" = sensibilidade,
                "F1-Score" = f1), format = "markdown", align = 'l')
metricas
```

#### Curva ROC

```{r ROC}
class1 <- predict(ajt.trn, newdata = mdl.vld, type = "response")
class2 <- mdl.vld$Survived

pred <- prediction(class1, class2)
perf <- performance(pred, "tpr", "fpr")
```

```{r Output ROC, fig.cap = 'Gerando uma curva ROC em R'}
# Gerando uma curva ROC em R
plot(perf, col = rainbow(10, alpha = NULL))
```

___
Realizando o ***Teste Chi-Quadrado*** (X²).
```{r Chi-Quadrado - Validação}
chisq.test(previsoes$observado, previsoes$previsto)
```

___
## Teste

Predição de teste utilizando novos dados.
```{r Predição para Teste}
predict(ajt.trn, 
        newdata = mdl.tst, 
        type = "response") %>% 
  round() %>% 
  factor(labels = c("Não", "Sim")) -> predicao
```

Tabela para as predições de teste.
```{r Tabela de Proporções - Teste}
CrossTable(predicao, prop.t = T, digits = 2)
```
